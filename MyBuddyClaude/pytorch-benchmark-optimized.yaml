apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: pytorch-benchmark-opt
  namespace: nccl-test
spec:
  serviceName: pytorch-benchmark-opt-svc
  replicas: 2
  selector:
    matchLabels:
      app: pytorch-benchmark-opt
  template:
    metadata:
      labels:
        app: pytorch-benchmark-opt
      annotations:
        k8s.v1.cni.cncf.io/networks: default/eno5np0-network,default/eno6np0-network,default/eno7np0-network,default/eno8np0-network
    spec:
      serviceAccountName: nccl-sa
      hostIPC: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - pytorch-benchmark-opt
            topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-H100-80GB-HBM3
      containers:
      - name: pytorch-benchmark-opt
        image: nvcr.io/nvidia/pytorch:24.12-py3
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -x
          echo "Starting OPTIMIZED PyTorch AllReduce Benchmark on pod ${POD_NAME}"

          # Extract rank from pod name
          export NODE_RANK=${POD_NAME##*-}
          export NNODES=2
          export NPROC_PER_NODE=4
          export MASTER_PORT=29501

          # Discover master pod IP
          if [ "${POD_NAME}" = "pytorch-benchmark-opt-0" ]; then
            export MASTER_ADDR=${POD_IP}
            echo "I am the master node (node_rank 0)"
          else
            echo "Discovering master pod IP..."
            for i in {1..60}; do
              MASTER_IP=$(getent hosts pytorch-benchmark-opt-0.pytorch-benchmark-opt-svc.nccl-test.svc.cluster.local | awk '{print $1}')
              if [ -n "$MASTER_IP" ]; then
                export MASTER_ADDR=$MASTER_IP
                echo "Master node IP: ${MASTER_ADDR}"
                break
              fi
              echo "Waiting for master pod... (attempt $i/60)"
              sleep 2
            done

            if [ -z "$MASTER_ADDR" ]; then
              echo "Failed to discover master node IP"
              exit 1
            fi
          fi

          echo "NODE_RANK=${NODE_RANK}, NNODES=${NNODES}, NPROC_PER_NODE=${NPROC_PER_NODE}"
          echo "MASTER_ADDR=${MASTER_ADDR}, MASTER_PORT=${MASTER_PORT}"
          echo "Total GPUs: $((NNODES * NPROC_PER_NODE))"

          # Display configuration
          echo "=== GPU Information ==="
          nvidia-smi --query-gpu=index,name,memory.total --format=csv

          echo "=== RDMA Devices ==="
          ls -la /dev/infiniband/ 2>/dev/null || echo "No InfiniBand devices"

          echo "=== SR-IOV Network Interfaces ==="
          for iface in net1 net2 net3 net4; do
            if [ -e /sys/class/net/$iface ]; then
              echo "$iface: $(cat /sys/class/net/$iface/address) MTU=$(cat /sys/class/net/$iface/mtu)"
            fi
          done

          echo "=== Optimized NCCL Environment ==="
          env | grep NCCL | sort

          # Wait for all pods to be ready
          echo "Waiting for all pods to be ready..."
          sleep 10

          echo "=== Starting OPTIMIZED PyTorch Distributed Benchmark ==="

          # Run the benchmark with torchrun
          torchrun \
            --nnodes=${NNODES} \
            --nproc_per_node=${NPROC_PER_NODE} \
            --node_rank=${NODE_RANK} \
            --master_addr=${MASTER_ADDR} \
            --master_port=${MASTER_PORT} \
            --rdzv_backend=c10d \
            --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
            /benchmark/allreduce-loop.py --multiplier 1

          BENCHMARK_EXIT_CODE=$?

          echo "=== Benchmark completed with exit code: ${BENCHMARK_EXIT_CODE} ==="

          if [ ${BENCHMARK_EXIT_CODE} -eq 0 ]; then
            echo "SUCCESS: Optimized benchmark completed on ${POD_NAME}"
          else
            echo "ERROR: Benchmark failed on ${POD_NAME}"
          fi

          # Keep container running for log inspection
          echo "Container will remain running. Use 'oc logs' to view results."
          sleep infinity
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

#        - name: NCCL_IB_DISABLE
#          value: "0"
#        - name: NCCL_DMABUF_ENABLE
#          value: "0"
#        - name: NCCL_IB_MERGE_NICS
#          value: "0"
#        - name: NCCL_IB_QPS_PER_CONNECTION
#          value: "2"
        # === OPTIMIZATION 1: Increase NCCL Channels ===
        # More channels = better utilization of 4 NICs
        - name: NCCL_MIN_NCHANNELS
          value: "8"
        - name: NCCL_MAX_NCHANNELS
          value: "16"

        # === OPTIMIZATION 2: Network Interface Configuration ===
        - name: NCCL_SOCKET_IFNAME
          value: "net1,net2,net3,net4"
        - name: NCCL_IB_HCA
          value: "mlx5_6,mlx5_7,mlx5_10,mlx5_11"

        # === OPTIMIZATION 3: Enhanced RDMA Settings ===
        - name: NCCL_NET_GDR_LEVEL
          value: "5"  # Maximum GPU Direct RDMA
        - name: NCCL_NET_GDR_READ
          value: "1"  # Enable RDMA read operations

        # === OPTIMIZATION 4: InfiniBand/RoCE Optimizations ===
        - name: NCCL_IB_GID_INDEX
          value: "3"
        - name: NCCL_IB_TC
          value: "106"
        - name: NCCL_IB_TIMEOUT
          value: "22"  # Increase IB timeout (default 18)
        - name: NCCL_IB_RETRY_CNT
          value: "7"   # Increase retry count
        - name: NCCL_IB_SL
          value: "0"   # Service Level
        - name: NCCL_IB_AR_THRESHOLD
          value: "8192"  # Adaptive routing threshold

        # === OPTIMIZATION 5: Protocol and Algorithm ===
        - name: NCCL_PROTO
          value: "Simple"  # Simple protocol (vs LL for large messages)
        - name: NCCL_ALGO
          value: "Ring,Tree"  # Enable both algorithms

        # === OPTIMIZATION 6: Buffer and Chunk Sizes ===
        - name: NCCL_BUFFSIZE
          value: "8388608"  # 8MB buffer (default 4MB)
        - name: NCCL_NTHREADS
          value: "640"  # Increase CUDA threads per channel (default 512)
        - name: NCCL_LL_THRESHOLD
          value: "0"  # Disable LL for all sizes (use Simple)
        - name: NCCL_TREE_THRESHOLD
          value: "0"  # Use Tree algorithm for all sizes

        # === OPTIMIZATION 7: Socket and Network Tuning ===
        - name: NCCL_SOCKET_FAMILY
          value: "4"  # IPv4
        - name: NCCL_NSOCKS_PERTHREAD
          value: "8"  # Increase sockets per thread (default 4)
        - name: NCCL_SOCKET_NTHREADS
          value: "8"  # Increase socket threads (default 4)

        # === OPTIMIZATION 8: Collective Operation Tuning ===
        - name: NCCL_CROSS_NIC
          value: "2"  # Enable cross-NIC communication
        - name: NCCL_NVLS_ENABLE
          value: "0"  # Disable NVSwitch (not available on this setup)
        - name: NCCL_NET_SHARED_BUFFERS
          value: "1"  # Enable shared buffers for network

        # === OPTIMIZATION 9: PXN (Packet eXtended Notation) ===
        - name: NCCL_IB_PCI_RELAXED_ORDERING
          value: "1"  # Enable PCIe relaxed ordering
        - name: NCCL_NET_OVERHEAD
          value: "0"  # Minimize network overhead estimation

        # === OPTIMIZATION 10: Topology Awareness ===
        - name: NCCL_TOPO_FILE
          value: ""  # Let NCCL auto-detect
        - name: NCCL_IGNORE_CPU_AFFINITY
          value: "0"  # Respect CPU affinity

        # === Debugging (can disable for production) ===
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_DEBUG_SUBSYS
          value: "INIT,NET"

        # === GPU Configuration ===
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"

        resources:
          requests:
            nvidia.com/gpu: 4
            openshift.io/eno5np0rdma: 1
            openshift.io/eno6np0rdma: 1
            openshift.io/eno7np0rdma: 1
            openshift.io/eno8np0rdma: 1
            cpu: "32"
            memory: "64Gi"
          limits:
            nvidia.com/gpu: 4
            openshift.io/eno5np0rdma: 1
            openshift.io/eno6np0rdma: 1
            openshift.io/eno7np0rdma: 1
            openshift.io/eno8np0rdma: 1
            cpu: "64"
            memory: "128Gi"
        volumeMounts:
        - name: benchmark-script
          mountPath: /benchmark
        - name: shm
          mountPath: /dev/shm
        securityContext:
          privileged: true
          capabilities:
            add:
            - IPC_LOCK
            - SYS_ADMIN
      volumes:
      - name: benchmark-script
        configMap:
          name: pytorch-benchmark
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
---
apiVersion: v1
kind: Service
metadata:
  name: pytorch-benchmark-opt-svc
  namespace: nccl-test
spec:
  clusterIP: None
  selector:
    app: pytorch-benchmark-opt
  ports:
  - port: 29501
    name: master
